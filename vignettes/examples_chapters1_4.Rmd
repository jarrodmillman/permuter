---
title: "Examples Chapters 1-4"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Vignette summary

```{r}
library(permuter)
```


## IPAT Data

```{r ipat}
data(ipat)
d = ipat$YA-ipat$YB
n = dim(ipat)[1]
B=1000
T<-array(0,dim=c((B+1),1))
T[1] = sum(d)
for(bb in 2:(B+1)){
T[bb] = t(d)%*%(1-2*rbinom(n,1,.5))
}
```

The array T contains has dimension (B + 1) × 1 and contains the null distribution of the test statistic. The first value is the observed value. In order to obtain a p-value we need to load the function t2p, that returns an array of p- values from an array of permutation values of the test statistic (i.e. it compares each element of T with the whole distribution according with the rule large is significant).

```{r show_t2p}
source("../R/t2p.r")
t2p(T)[1]
```

The first element of the output is the p-value of this analysis. In this case the null hypothesis $H0 : Y_1 =d Y_2$ is rejected if favor of the alternative $H_1: Y_1 >d Y_2$.  Let's calculate the conditional power.

```{r conditional_power}
R=1000
p.val =array(0,dim=c(R,1))
Z = ipat[,1]-ipat[,2]-sum(d)
for(cc in 1:R){
  Z.star=sample(Z)
  Z.star = Z.star + sum(d)
  T<-array(0,dim=c((B+1),1))
  T[1] = sum(d)
  for(bb in 2:(B+1)){
    T[bb] = t(d)%*%(1-2*rbinom(n,1,.5))
  }
  p.val[cc]=t2p(T)[1]
}

alpha = 0.0005
pow = mean(p.val <= alpha)
pow
```

## Job Satisfaction


```{r jobsatis1}
data(job)
head(job)
Y = job[,1]
X = job[,2]
```
In the "Job"" dataset there are two variables: $X$ denoting the degree of job satisfaction and $Y$ denoting the extroverted (1) or introverted (2) group. We can obtain the test statistic $T^∗ = \bar{X}_1^∗ − \bar{X}_2^∗$ by multiplying the vector of permuted data $x^∗$ for a vector of constrasts:

```{r jobsatis2}
n = table(Y)
C = length(n)
contr = rep(1/n,n)
contr[-c(1:n[1])] = -contr[-c(1:n[1])]
round(contr,digits=3)
```

```{r jobsatis3}
B = 100
T <- array(0,dim=c((B+1),1))
T[1] = X%*%contr
for(bb in 2:(B+1)){
  X.star = sample(X)
  T[bb] = X.star%*%contr
}

P=t2p(T); P[1]
```

### Confidence interval
```{r jobsatis_confint}
IC(X[Y==1],X[Y==2],conf.lev=0.95,max.delta=10,length.delta=500)
```

### Conditional Power 
We'll use the IC function. In the book it is obtained as in "IPAT Data" example.
```{r jobsatis_power}
MB=100
m = T[1]
ICr = array(0,dim=c(B,2))
Z = c(X[Y==1]-m,X[Y==2])
for(bb in 1:MB){
  Z.perm=sample(Z)
  Z.perm[Y==1]=Z.perm[Y==1]+m
  ICr[bb,]=IC(Z.perm[Y==1],Z.perm[Y==2],conf.lev=0.95,max.delta=10,length.delta=100)
}
pow = 1-mean(ICr[,1]<0 & ICr[,2]>0);
pow
```


## Worms
This is an example of one way ANOVA analysis, where the factor is given by the group Y and data are the lengths X of the worms belonging to each group.

```{r worms1}
data(worms)
Y <- worms[,1]
X <- worms[,2]
n = table(Y)
C = length(n)
n
```

It is easy to show that, conditionally, the usual F test statistic for the one-way ANOVA is permutationally equivalent to $T^{obs} = \sum_{j=1}^3 n_j \bar{X}_j^2$. We can obtain the distribution of the test statistic by computing, at each permutation, the values of $\bar{X}_j^2, j=1,2,3$. This can be done with the auxiliary dummy variables $I$:

```{r worms2}
I = array(0,dim=c(sum(n),C))
for(i in 1:C){
I[,i]<-ifelse(Y==names(n)[i],1/n[i],0)
}
```

The $i$th element of the $j$th column of matrix $I$ is equal to $1/n_j$ if the $i$th
observation belongs to group $j$ and zero otherwise, $i = 1,...,N$, $N = \sum_j n_j$,
$j = 1,2,3$. Thus the vector of means-by-group $\bar{x} = [\bar{x}_1, \bar{x}_2,\bar{x}_3]$ can be easily derived by multiplying $t(X)$ (with dimension $1 \times N$) and $I$ (with dimension $N \times 3$), and the test statistic can be derived by multiplying the square of $\bar{x}$ and
the vector $n$. The permutation values of $T^* = \sum_{j=1}^3n_j {\bar{X}_j^*}^2$ can be obtained similarly by replacing $X$ with $X.star$, a random permutation of $X$:

```{r worms3}
T = array(0,dim=c((B+1),1))
T[1] = ( t(X)%*%I )^2%*%n
for(bb in 2:(B+1)){
  X.star=sample(X)
  T[bb] = ( t(X.star)%*%I )^2%*%n
}
t2p(T)[1]
```

The p-value of the test is again obtained by applying the t2p function to the vector $T$, and by considering the first element of the vector of results. In this case we can conclude that there is a strong evidence against the null hypothesis.

## Anderson-Darling
The data of Table 1 can be obtained by inserting the absolute frequencies of groups 1 and 2, obtaining their cumulative sum and binding together the four vectors:

**to do: use xtable to generate Table 1 nicely**

```{r ad}
f1 = c(8,9,6,8,9) ; f2 = c(17,6,6,3,8);
N1 = cumsum(f1) ; N2 = cumsum(f2);
N = f1+f2
n = sum(N);
cbind(f1,f2,N1,N2,N)
```

The test statistic $T_D^*$ is a sum over $k-1 = 4$ categories of the quantities $D_i = N_{2i}/[N_{.i}\times(n-N_{.i})]^{1/2}$, $i=1,\dots,4$. It is then easier to create a vector containing the $D_i$'s and then sum its elements to obtain $T_D^*$. For the observed data:

```{r ad2}
B=100
T = array(0,dim=c((B+1),1))
D = N2/sqrt(N*(n-N))
T[1] = sum(D[1:4])
```

A random permutation of data can be obtained by re-creating the original data that gives the observed frequencies f1 and f2. Indeed, we have to obtain all pos- sible configurations of frequencies $\{f_{i1},f_{i2}\}$ that satisfies the row and columns totals. To do that, first we create the vectors X1 and X2 containing the categories of each observation in each sample (for semplicity we indicate the categories with the numbers from one to five). Later on, concatenate the vectors X1 and X2 in the vector X and let X.star be a random permutation of X. Create a binary vector of group labels Y. In this example X1 and X2 are vectors of lengths 40, X and Y have lengths equal to 80. X1 and X2 are such that table(X1) = f1 and table(X2) = f2.

Finally, the frequency table corresponding to a random permutation can be obtained by applying the function table to the elements of $X.star$ belonging to the first and second sample, respectively. The permutation values of the test statistic are then obtained as above. Note that this way of proceeding guarantees that the marginal distributions of Table 1 are fixed, therefore we only need to obtain the frequency distribution in the second sample.

```{r ad3}
X1<-rep(seq(1,5),f1)
X2<-rep(seq(1,5),f2)
X<-c(X1,X2)
Y <-rep(c(1,2),c(sum(f1),sum(f2)))
options(warn=-1)
for(bb in 2:(B+1)){
  X.star=sample(X)
  f2.star = table(X.star[Y==2])
  N2.star = cumsum(f2.star)
  D.star = N2.star/sqrt(N*(n-N))
  T[bb] = sum(D.star[1:4])
}
t2p(T)[1]
```

## Blood testosterone levels in 11 women

The purpose was to evaluate whether the level of testosterone in the blood is subject to change during the day. This example has the peculiarity that the ob- servations are dependent since they are related to different women. Therefore, under H0 we can permute the data inside the rows of the data-set indepen- dently. This problem can be viewed as a two-way ANOVA experiment without interaction, where main factors are ”time” (factor B) and ”woman” (factor A: blocking factor, not of interest).

```{r testosterone1}
data(testosterone)
Y = rep(seq(1,5),each=11)
Time = colnames(testosterone)
boxplot(unlist(testosterone)~Y,xlab="Time",ylab="Testosterone",names=Time)
lines(seq(1,5),apply(testosterone,2,mean),lty="dotted")
```

The commands above assign the dataset to the object +testosterone+ and represent it with a box-plot, the dotted line linking the sample means at each time. The deviance decomposition can be written as $SST = SSA + SSB + SSR$, where $SST$ is the total deviance, $SSA$ and $SSB$ are the deviances due to the main effects and $SSR$ is the residual deviance. Note that $SST$ is constant at each permutation, and so is $SSA$ since we permute observations within the rows of data. On the other hand, $SSB$ and $SSR$ vary at each permutation. The test statistic for the time effect is $F_B = (df_{SSR}/df_{SSB}) \times SSB/SSR$. Leaving out the degrees of freedom $df_{SSB}$ and $df_{SSR}$ that are permutationally invariant, the easiest way to obtain the residual deviance at each permutation is to write it as $SSR^* = SST − SSA − SSB^*$. Therefore, the F statistic can be written as $F^* = SSB^*/(SS − SSB^*)$, where $SS$ is constant at each permutation. It is easy to see that $F^*$ is a monotone function of $T^* = SSB^*$.

```{r testosterone2}
n=dim(testosterone)[1]
p = dim(testosterone)[2]
B=100
m = mean(mean(testosterone))
m.col = apply(testosterone,2,mean)
SSB = n*sum((m.col-m)^2)
T<-array(0,dim=c((B+1),1))
T[1] = SSB
data.star = data
for(bb in 2:(B+1)){
  U = matrix(runif(n*p),nrow=n) ## U is n x p
  R = apply(U,1,rank)           ## R is p x n
  for(i in 1:n){
    data.star[i,] = data[i,R[,i]]
  }
  m.col = apply(data.star,2,mean)
  SSB =  n*sum((m.col-m)^2)
  T[bb] = SSB
}
t2p(T)[1]
```


## Biting flies

In this example we have two samples from two species of flies and seven variables have been measured. We want to test for rescrited alternative:

$$H_1: \left\lbrace \left\[ \cup_{1\leq h \leq 6} (\mu_{1h}<\mu_{2h}) \cup (\mu_{17}>\mu_{27}) \right] \right\rbrace$$

where $\mu_{ih}$ is the mean of the $h$th variable in group $i$, $i=1,2$, $h=1,\dots,7$. This can be done by performing one partial test for each variable (according to the related alternative), and then by combining the partial tests into a global test. A vector of contrasts will be again useful in obtaining the test statistic $T_h^* = \bar{x}_{2h}^* - \bar{x}_{1h}^*$.  The first column of the dataset contains an indicator variable of the species (0=Leptoconops Carteri, 1=Leptoconops Torrens). Since the t2p function is set according to the *large is significant rule*, we will have to change the signs of the permutation distribution of $T_7^*$ before obtaining its p-value.

```{r flies1}
data(fly)
N = dim(fly)[1]
p = dim(fly)[2]-1
n = table (fly[,1])
B = 100
contr = as.vector(rep(c(-1/n[1],1/n[2]),each=n))
data = as.matrix(fly[,-1])
T<-array(0,dim=c((B+1),p))
T[1,] = t(contr)%*%fly
for(bb in 2:(B+1)){
  fly.star = fly[sample(1:N),]
  T[bb,] = t(contr)%*%fly.star
}
T[,7] = -T[,7]
P = t2p(T)
partial.p = P[1,] ; names(partial.p) = colnames(fly);
round(partial.p,digits=4)
```

The vector $partial.p$ contains the partial p-values of each variable. The matrix $P$ contains the null distribution of partial p-values. It results that X1, X3 and X5 are strongly significant and X7 is moderately significant. We combine the partial tests with Fisher’s, Liptak’s, and Tippett’s combining functions. Note that if Tippett’s combining function is applied, the rule is small is significant, therefore we run the t2p function to the inverse of the vector containing Tippett’s combination of the rows of $P$.

** NB the naming convention here is very bad -- in R, F means false and T means true!**
```{r fly2}
F = apply(P,1,function(x){-2*log(prod(x))})
L = apply(P,1,function(x){sum(qnorm(1-x))})
T = apply(P,1,min)
P.F = t2p(F)
P.L = t2p(L)
P.T = t2p(1/T);
globs = c(P.F[1],P.L[1],P.T[1])
names(globs) = c("Fisher","Liptak","Tippett")
globs
```

